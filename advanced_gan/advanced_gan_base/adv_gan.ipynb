{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSZFxen9A_wF"
      },
      "source": [
        "# Advanced GAN Base Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xej_QDNEA_wG"
      },
      "source": [
        "Building a base Advanced Gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "os.makedirs(\"celeba_gan\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")\n",
        "\n",
        "\n",
        "dataset = keras.utils.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)\n",
        "\n",
        "\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy import expand_dims, ones, zeros, vstack\n",
        "from numpy.random import rand, randint, randn\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout, LeakyReLU, Conv2DTranspose, Reshape\n",
        "from matplotlib import pyplot\n",
        "import logging\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "def initialize_logger():\n",
        "    \"\"\"\n",
        "    Creates a logger for hyperparameters and training data.\n",
        "    \"\"\"\n",
        "    # Get the name of the current script\n",
        "    script_name = 'dcgan_base'\n",
        "    log_filename = f'{script_name}.log'\n",
        "\n",
        "    # Create a custom logger\n",
        "    logger = logging.getLogger(script_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Create handlers\n",
        "    c_handler = logging.StreamHandler()\n",
        "    f_handler = logging.FileHandler(log_filename)\n",
        "    c_handler.setLevel(logging.INFO)\n",
        "    f_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Create formatters and add it to handlers\n",
        "    c_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
        "    f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    c_handler.setFormatter(c_format)\n",
        "    f_handler.setFormatter(f_format)\n",
        "\n",
        "    # Add handlers to the logger\n",
        "    logger.addHandler(c_handler)\n",
        "    logger.addHandler(f_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "def log_model_summary(model, logger):\n",
        "    with io.StringIO() as buf, redirect_stdout(buf):\n",
        "        model.summary()\n",
        "        summary = buf.getvalue()\n",
        "    logger.info(summary)\n",
        "\n",
        "\n",
        "def create_discriminator(in_shape=(28,28,1)):\n",
        "    \"\"\"\n",
        "    Creates a discrimator model\n",
        "\n",
        "    Input:\n",
        "    in-shape: This is the shape of the photos that will be put into the discriminator model.\n",
        "\n",
        "    Output:\n",
        "    The model for discriminating fake vs real images\n",
        "    \"\"\"\n",
        "    model =  Sequential()\n",
        "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile\n",
        "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    \"\"\"\n",
        "    This function loads the MNIST dataset, and scales it to be in a sigmoid (0, 1) range\n",
        "    \"\"\"\n",
        "    (trainX, _), (_, _) = load_data()\n",
        "    # add third dimension for color value\n",
        "    X = expand_dims(trainX, axis=-1)\n",
        "    X = X.astype('float32')\n",
        "    X = X / 255.0\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def select_real_samples(dataset, n_samples):\n",
        "    \"\"\"\n",
        "    Selects some number of real samples to train with from the input dataset.\n",
        "    Uses random to select random images. Labels the images as 'real' with label = 1\n",
        "\n",
        "    Inputs:\n",
        "    dataset: Input dataset(MNIST)\n",
        "    n_sample: number of samples to select\n",
        "\n",
        "    Return:\n",
        "    X: Selected images\n",
        "    y: tags for images\n",
        "    \"\"\"\n",
        "    # get n number of random images from dataset\n",
        "    i = randint(0, dataset.shape[0], n_samples)\n",
        "\n",
        "    X = dataset[i]\n",
        "    y = ones((n_samples, 1))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "def initial_create_fake_samples(n_samples):\n",
        "    \"\"\"\n",
        "    Creates fake samples to train discriminator with correct dimensions\n",
        "\n",
        "    Input:\n",
        "    n_samples: number of samples to create\n",
        "\n",
        "    Return:\n",
        "    X: fake images\n",
        "    y: image tags for training, 0 to mean not real images\n",
        "    \"\"\"\n",
        "    X = rand(28 * 28 * n_samples)\n",
        "\n",
        "    # Reshape and add tags to show fake\n",
        "    X = X.reshape((n_samples, 28, 28, 1))\n",
        "    y = zeros((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def train_discriminator(model, dataset, iterations=100, batch_size=256):\n",
        "    \"\"\"\n",
        "    Trains the discriminator using mnist dataset and fake images.\n",
        "    Takes half batch size of real and fake for each iteration.\n",
        "\n",
        "    Inputs:\n",
        "    model: input model\n",
        "    dataset: loaded dataset (MNIST)\n",
        "    iterations: number of iterations of training\n",
        "    batch_size: images to train with in each iteration\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Select real images and train discriminator\n",
        "        X_real, y_real = select_real_samples(dataset, int(batch_size / 2))\n",
        "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
        "\n",
        "        # Select fake images and train discriminator\n",
        "        X_fake, y_fake = initial_create_fake_samples(int(batch_size / 2))\n",
        "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "        #Performance\n",
        "        print(f'>{i+1} real={real_acc*100:.0f}% fake={fake_acc*100:.0f}%')\n",
        "\n",
        "\n",
        "def create_generator(latent_dim):\n",
        "    \"\"\"\n",
        "    Creates a generator model.\n",
        "    Starts with a 7x7 and reshapes to be 14x14 then 28x28.\n",
        "\n",
        "    Input:\n",
        "    latent-dim: Dimension of the latent space\n",
        "\n",
        "    Return:\n",
        "    The generator model\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    n_nodes = 128 * 7 * 7\n",
        "\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    \"\"\"\n",
        "    Generates points in latent space.\n",
        "    Used as input for the generator\n",
        "\n",
        "    Inputs:\n",
        "    latent_dim: Dimension of the latent space\n",
        "    n_samples: number of samples to generate\n",
        "\n",
        "    Return:\n",
        "    x_input: points in latent space\n",
        "    \"\"\"\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "\n",
        "def create_fake_samples(g_model, latent_dim, n_samples):\n",
        "    \"\"\"\n",
        "    Generates fake samples from the generator model.\n",
        "    Creates labels of 0 for the fake images\n",
        "\n",
        "    Inputs:\n",
        "    g_model: generator model\n",
        "    latent_dim: Dimension of the latent space\n",
        "    n_samples: number of samples to generate\n",
        "\n",
        "    Return:\n",
        "    X: fake images\n",
        "    y: image tags for training, 0 to mean not real\n",
        "    \"\"\"\n",
        "    x_input = generate_latent_points(latent_dim, n_samples)\n",
        "    X = g_model.predict(x_input)\n",
        "    y = zeros((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def create_gan(g_model, d_model):\n",
        "    \"\"\"\n",
        "    Creating the GAN model. The generator is trained but the discriminator is untrainable.\n",
        "\n",
        "    Inputs:\n",
        "    g_model: generator model\n",
        "    d_model: discriminator model\n",
        "\n",
        "    Return:\n",
        "    The GAN model\n",
        "    \"\"\"\n",
        "    d_model.trainable = False\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(g_model)\n",
        "    model.add(d_model)\n",
        "\n",
        "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
        "    \"\"\"\n",
        "    Trains the generator and discriminator.\n",
        "\n",
        "    Inputs:\n",
        "    g_model: generator model\n",
        "    d_model: discriminator model\n",
        "    gan_model: GAN model\n",
        "    dataset: MNIST dataset\n",
        "    latent_dim: Dimension of the latent space\n",
        "    n_epochs: number of epochs to train\n",
        "    n_batch: number of images to train with in each iteration\n",
        "    \"\"\"\n",
        "    num_batch = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "\n",
        "        for j in range(num_batch):\n",
        "            # generate real and fake samples\n",
        "            X_real, y_real = select_real_samples(dataset, half_batch)\n",
        "            X_fake, y_fake = create_fake_samples(g_model, latent_dim, half_batch)\n",
        "\n",
        "            # train discriminator separately\n",
        "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
        "            d_loss, _ = d_model.train_on_batch(X, y)\n",
        "\n",
        "            # train generator - use 1 value to find how often it is wrong to update model\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            y_gan = ones((n_batch, 1))\n",
        "\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            print(f'>{i+1}, {j+1}/{num_batch}, d={d_loss:.3f}, g={g_loss:.3f}')\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            performance_summary(i, g_model, d_model, dataset, latent_dim)\n",
        "\n",
        "\n",
        "def create_plot(examples, epoch, n=10):\n",
        "    \"\"\"\n",
        "    Creates a physical picture to look at to see how the training is coming after each 10 epochs\n",
        "\n",
        "    Inputs:\n",
        "    examples: images to plot\n",
        "    epoch: epoch number\n",
        "    n: number of images to plot\n",
        "    \"\"\"\n",
        "    for i in range(n * n):\n",
        "        pyplot.subplot(n, n, 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "    filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "    pyplot.savefig(filename)\n",
        "    pyplot.close()\n",
        "\n",
        "\n",
        "def performance_summary(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
        "    \"\"\"\n",
        "    Every 10 epochs, save a copy of the model as well as a plot of generated images.\n",
        "\n",
        "    Inputs:\n",
        "    epoch: epoch number\n",
        "    g_model: generator model\n",
        "    d_model: discriminator model\n",
        "    dataset: MNIST dataset\n",
        "    latent_dim: Dimension of the latent space\n",
        "    n_samples: number of samples to generate\n",
        "    \"\"\"\n",
        "    # take real samples and evaluate discriminator\n",
        "    X_real, y_real = select_real_samples(dataset, n_samples)\n",
        "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\n",
        "    # take generated samples and evaluate using discriminator\n",
        "    x_fake, y_fake = create_fake_samples(g_model, latent_dim, n_samples)\n",
        "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\n",
        "    # summarize discriminator performance\n",
        "    print(f'>Accuracy real: {acc_real*100:.0f}%, fake: {acc_fake*100:.0f}%')\n",
        "    logger.info(f'>Accuracy real: {acc_real*100:.0f}%, fake: {acc_fake*100:.0f}%')\n",
        "\n",
        "    # create the 10 x 10 plot picture\n",
        "    create_plot(x_fake, epoch)\n",
        "\n",
        "    # save the generator model tile file\n",
        "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
        "    g_model.save(filename)\n",
        "\n",
        "\n",
        "# load and prepare mnist training images\n",
        "def load_real_samples():\n",
        "    \"\"\"\n",
        "    Loads the MNIST dataset and scales it to be in a sigmoid (0, 1) range\n",
        "    \"\"\"\n",
        "    (trainX, _), (_, _) = load_data()\n",
        "    X = expand_dims(trainX, axis=-1)\n",
        "    X = X.astype('float32')\n",
        "    X = X / 255.0\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "logger = initialize_logger()\n",
        "logger.info('Program started')\n",
        "logger.info('------------------------------------------------------------')\n",
        "\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = create_discriminator()\n",
        "logger.info('Discriminator created')\n",
        "log_model_summary(d_model, logger)\n",
        "logger.info('------------------------------------------------------------')\n",
        "# create the generator\n",
        "g_model = create_generator(latent_dim)\n",
        "logger.info('Generator created')\n",
        "log_model_summary(g_model, logger)\n",
        "logger.info('------------------------------------------------------------')\n",
        "# create the gan\n",
        "gan_model = create_gan(g_model, d_model)\n",
        "logger.info('GAN created')\n",
        "log_model_summary(gan_model, logger)\n",
        "logger.info('------------------------------------------------------------')\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "logger.info('Training started')\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim)\n",
        "\n",
        "logger.info('Training finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
