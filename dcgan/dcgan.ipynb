{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims, ones, zeros\n",
    "from numpy.random import rand, randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to build a discriminator model. This model will take a 28X28x1 image and down sample the image to 14x14 and then to 7x7. The reason for the extra dimension is to give a space for the value of the pixel (white or black, but in a sigmoid 0 to 1 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(in_shape=(28,28,1)):\n",
    "    \"\"\"\n",
    "    Creates a discrimator model\n",
    "    \n",
    "    Input:\n",
    "    in-shape: This is the shape of the photos that will be put into the discriminator model.\n",
    "    \n",
    "    Output:\n",
    "    The model for discriminating fake vs real images\n",
    "    \"\"\"\n",
    "    model =  Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mnist training images and generate fake images to train discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist data set\n",
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    This function loads the MNIST dataset, and scales it to be in a sigmoid (0, 1) range\n",
    "    \"\"\"\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    # add third dimension for color value\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    X = X.astype('float32')\n",
    "    X = X / 255.0\n",
    "    return X\n",
    " \n",
    "# Randomly select real samples from MNIST to train with\n",
    "def select_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Selects some number of real samples to train with from the input dataset.\n",
    "    Uses random to select random images. Labels the images as 'real' with label = 1\n",
    "    \n",
    "    Inputs:\n",
    "    dataset: Input dataset(MNIST)\n",
    "    n_sample: number of samples to select\n",
    "    \n",
    "    Return:\n",
    "    X: Selected images\n",
    "    y: tags for images\n",
    "    \"\"\"\n",
    "    # get n number of random images from dataset\n",
    "    i = randint(0, dataset.shape[0], n_samples)\n",
    "    \n",
    "    X = dataset[i]\n",
    "    y = ones((n_samples, 1))\n",
    "    \n",
    "    return X, y\n",
    " \n",
    "# Create n amount of fake images to train discriminator\n",
    "def create_fake_samples(n_samples):\n",
    "    \"\"\"\n",
    "    Creates fake samples to train discriminator with correct dimensions\n",
    "    \n",
    "    Input:\n",
    "    n_samples: number of samples to create\n",
    "    \n",
    "    Return:\n",
    "    X: fake images\n",
    "    y: image tags for training, 0 to mean not real images\n",
    "    \"\"\"\n",
    "    X = rand(28 * 28 * n_samples)\n",
    "    \n",
    "    # Reshape and add tags to show fake\n",
    "    X = X.reshape((n_samples, 28, 28, 1))\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 real=39% fake=47%\n",
      ">2 real=42% fake=64%\n",
      ">3 real=47% fake=73%\n",
      ">4 real=52% fake=88%\n",
      ">5 real=49% fake=91%\n",
      ">6 real=50% fake=98%\n",
      ">7 real=61% fake=98%\n",
      ">8 real=61% fake=100%\n",
      ">9 real=61% fake=100%\n",
      ">10 real=66% fake=100%\n",
      ">11 real=73% fake=100%\n",
      ">12 real=67% fake=100%\n",
      ">13 real=77% fake=100%\n",
      ">14 real=74% fake=100%\n",
      ">15 real=75% fake=100%\n",
      ">16 real=77% fake=100%\n",
      ">17 real=82% fake=100%\n",
      ">18 real=88% fake=100%\n",
      ">19 real=81% fake=100%\n",
      ">20 real=88% fake=100%\n",
      ">21 real=91% fake=100%\n",
      ">22 real=89% fake=100%\n",
      ">23 real=95% fake=100%\n",
      ">24 real=91% fake=100%\n",
      ">25 real=92% fake=100%\n",
      ">26 real=95% fake=100%\n",
      ">27 real=95% fake=100%\n",
      ">28 real=99% fake=100%\n",
      ">29 real=98% fake=100%\n",
      ">30 real=99% fake=100%\n",
      ">31 real=100% fake=100%\n",
      ">32 real=99% fake=100%\n",
      ">33 real=98% fake=100%\n",
      ">34 real=99% fake=100%\n",
      ">35 real=100% fake=100%\n",
      ">36 real=100% fake=100%\n",
      ">37 real=100% fake=100%\n",
      ">38 real=99% fake=100%\n",
      ">39 real=98% fake=100%\n",
      ">40 real=100% fake=100%\n",
      ">41 real=100% fake=100%\n",
      ">42 real=100% fake=100%\n",
      ">43 real=100% fake=100%\n",
      ">44 real=100% fake=100%\n",
      ">45 real=100% fake=100%\n",
      ">46 real=100% fake=100%\n",
      ">47 real=100% fake=100%\n",
      ">48 real=100% fake=100%\n",
      ">49 real=100% fake=100%\n",
      ">50 real=100% fake=100%\n",
      ">51 real=100% fake=100%\n",
      ">52 real=100% fake=100%\n",
      ">53 real=100% fake=100%\n",
      ">54 real=100% fake=100%\n",
      ">55 real=100% fake=100%\n",
      ">56 real=100% fake=100%\n",
      ">57 real=100% fake=100%\n",
      ">58 real=100% fake=100%\n",
      ">59 real=100% fake=100%\n",
      ">60 real=100% fake=100%\n",
      ">61 real=100% fake=100%\n",
      ">62 real=100% fake=100%\n",
      ">63 real=100% fake=100%\n",
      ">64 real=100% fake=100%\n",
      ">65 real=100% fake=100%\n",
      ">66 real=100% fake=100%\n",
      ">67 real=100% fake=100%\n",
      ">68 real=100% fake=100%\n",
      ">69 real=100% fake=100%\n",
      ">70 real=100% fake=100%\n",
      ">71 real=100% fake=100%\n",
      ">72 real=100% fake=100%\n",
      ">73 real=100% fake=100%\n",
      ">74 real=100% fake=100%\n",
      ">75 real=100% fake=100%\n",
      ">76 real=100% fake=100%\n",
      ">77 real=100% fake=100%\n",
      ">78 real=100% fake=100%\n",
      ">79 real=100% fake=100%\n",
      ">80 real=100% fake=100%\n",
      ">81 real=100% fake=100%\n",
      ">82 real=100% fake=100%\n",
      ">83 real=100% fake=100%\n",
      ">84 real=100% fake=100%\n",
      ">85 real=100% fake=100%\n",
      ">86 real=100% fake=100%\n",
      ">87 real=100% fake=100%\n",
      ">88 real=100% fake=100%\n",
      ">89 real=100% fake=100%\n",
      ">90 real=100% fake=100%\n",
      ">91 real=100% fake=100%\n",
      ">92 real=100% fake=100%\n",
      ">93 real=100% fake=100%\n",
      ">94 real=100% fake=100%\n",
      ">95 real=100% fake=100%\n",
      ">96 real=100% fake=100%\n",
      ">97 real=100% fake=100%\n",
      ">98 real=100% fake=100%\n",
      ">99 real=100% fake=100%\n",
      ">100 real=100% fake=100%\n"
     ]
    }
   ],
   "source": [
    "def train_discriminator(model, dataset, iterations=100, batch_size=256):\n",
    "    \"\"\"\n",
    "    Trains the discriminator using mnist dataset and fake images.\n",
    "    Takes half batch size of real and fake for each iteration.\n",
    "    \n",
    "    Inputs:\n",
    "    model: input model\n",
    "    dataset: loaded dataset (MNIST)\n",
    "    iterations: number of iterations of training\n",
    "    batch_size: images to train with in each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Select real images and train discriminator\n",
    "        X_real, y_real = select_real_samples(dataset, int(batch_size / 2))\n",
    "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
    "        \n",
    "        # Select fake images and train discriminator\n",
    "        X_fake, y_fake = create_fake_samples(int(batch_size / 2))\n",
    "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "        #Performance\n",
    "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))\n",
    " \n",
    "model = create_discriminator()\n",
    "dataset = load_mnist()\n",
    "train_discriminator(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that with fake images as messy as randomly generated images along with real images from the mnist data set, we didn't need that many iterations or very big batch size to create a discriminator which could detect fakes pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 1/234, d=0.686, g=0.717\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 2/234, d=0.684, g=0.724\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 3/234, d=0.682, g=0.728\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 4/234, d=0.680, g=0.732\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 5/234, d=0.678, g=0.736\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 6/234, d=0.675, g=0.740\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 7/234, d=0.675, g=0.745\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 8/234, d=0.675, g=0.749\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 9/234, d=0.672, g=0.753\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 10/234, d=0.671, g=0.756\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 11/234, d=0.669, g=0.760\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 12/234, d=0.667, g=0.763\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 13/234, d=0.665, g=0.767\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 14/234, d=0.664, g=0.770\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 15/234, d=0.663, g=0.773\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 16/234, d=0.659, g=0.776\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 17/234, d=0.659, g=0.779\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 18/234, d=0.661, g=0.781\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 19/234, d=0.657, g=0.783\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 20/234, d=0.656, g=0.784\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 21/234, d=0.656, g=0.786\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 22/234, d=0.657, g=0.786\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 23/234, d=0.654, g=0.787\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 24/234, d=0.658, g=0.787\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 25/234, d=0.656, g=0.786\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 26/234, d=0.653, g=0.785\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 27/234, d=0.655, g=0.784\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 28/234, d=0.654, g=0.782\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 29/234, d=0.654, g=0.780\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 30/234, d=0.654, g=0.778\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 31/234, d=0.654, g=0.775\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 32/234, d=0.656, g=0.772\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 33/234, d=0.658, g=0.770\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 34/234, d=0.660, g=0.766\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 35/234, d=0.657, g=0.763\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 36/234, d=0.660, g=0.759\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 37/234, d=0.658, g=0.756\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 38/234, d=0.658, g=0.752\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 39/234, d=0.656, g=0.749\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 40/234, d=0.658, g=0.746\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 41/234, d=0.660, g=0.743\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 42/234, d=0.658, g=0.740\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 43/234, d=0.658, g=0.737\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 44/234, d=0.656, g=0.734\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 45/234, d=0.658, g=0.732\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 46/234, d=0.655, g=0.729\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 47/234, d=0.659, g=0.727\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 48/234, d=0.658, g=0.726\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 49/234, d=0.653, g=0.723\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 50/234, d=0.654, g=0.722\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 51/234, d=0.654, g=0.721\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 52/234, d=0.653, g=0.719\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 53/234, d=0.649, g=0.718\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 54/234, d=0.650, g=0.717\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 55/234, d=0.647, g=0.716\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 56/234, d=0.649, g=0.715\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 57/234, d=0.648, g=0.713\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 58/234, d=0.643, g=0.713\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 59/234, d=0.644, g=0.712\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 60/234, d=0.640, g=0.711\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 61/234, d=0.642, g=0.710\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 62/234, d=0.637, g=0.709\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 63/234, d=0.638, g=0.709\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 64/234, d=0.637, g=0.708\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 65/234, d=0.633, g=0.708\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 66/234, d=0.632, g=0.707\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 67/234, d=0.634, g=0.707\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 68/234, d=0.629, g=0.706\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 69/234, d=0.627, g=0.706\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 70/234, d=0.627, g=0.706\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 71/234, d=0.624, g=0.705\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 72/234, d=0.622, g=0.705\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 73/234, d=0.622, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 74/234, d=0.618, g=0.704\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 75/234, d=0.618, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 76/234, d=0.615, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 77/234, d=0.616, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 78/234, d=0.608, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 79/234, d=0.607, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 80/234, d=0.609, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 81/234, d=0.605, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 82/234, d=0.601, g=0.703\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 83/234, d=0.597, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 84/234, d=0.598, g=0.703\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 85/234, d=0.595, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 86/234, d=0.593, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 87/234, d=0.594, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 88/234, d=0.593, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 89/234, d=0.584, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 90/234, d=0.586, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 91/234, d=0.584, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 92/234, d=0.580, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 93/234, d=0.580, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 94/234, d=0.576, g=0.703\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 95/234, d=0.574, g=0.703\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 96/234, d=0.573, g=0.704\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 97/234, d=0.567, g=0.704\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 98/234, d=0.567, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 99/234, d=0.564, g=0.704\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 100/234, d=0.560, g=0.705\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 101/234, d=0.556, g=0.705\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 102/234, d=0.555, g=0.705\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 103/234, d=0.557, g=0.705\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 104/234, d=0.550, g=0.706\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 105/234, d=0.549, g=0.706\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 106/234, d=0.544, g=0.707\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 107/234, d=0.545, g=0.707\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 108/234, d=0.540, g=0.707\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 109/234, d=0.537, g=0.708\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 110/234, d=0.539, g=0.708\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 111/234, d=0.526, g=0.708\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 112/234, d=0.530, g=0.709\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 113/234, d=0.528, g=0.709\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 114/234, d=0.525, g=0.709\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 115/234, d=0.525, g=0.710\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 116/234, d=0.516, g=0.710\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 117/234, d=0.511, g=0.711\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 118/234, d=0.512, g=0.711\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 119/234, d=0.505, g=0.712\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 120/234, d=0.509, g=0.712\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 121/234, d=0.511, g=0.713\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 122/234, d=0.504, g=0.713\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 123/234, d=0.489, g=0.714\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 124/234, d=0.498, g=0.715\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 125/234, d=0.495, g=0.715\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 126/234, d=0.493, g=0.716\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 127/234, d=0.485, g=0.716\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 128/234, d=0.482, g=0.717\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 129/234, d=0.478, g=0.717\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 130/234, d=0.483, g=0.718\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 131/234, d=0.476, g=0.719\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 132/234, d=0.476, g=0.720\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 133/234, d=0.474, g=0.720\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 134/234, d=0.472, g=0.721\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 135/234, d=0.472, g=0.722\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 136/234, d=0.464, g=0.722\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 137/234, d=0.469, g=0.723\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 138/234, d=0.466, g=0.724\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 139/234, d=0.461, g=0.725\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 140/234, d=0.461, g=0.726\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 141/234, d=0.456, g=0.727\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 142/234, d=0.451, g=0.727\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 143/234, d=0.446, g=0.728\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 144/234, d=0.449, g=0.729\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 145/234, d=0.445, g=0.730\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 146/234, d=0.444, g=0.731\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 147/234, d=0.435, g=0.732\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 148/234, d=0.441, g=0.734\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 149/234, d=0.434, g=0.735\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 150/234, d=0.434, g=0.736\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 151/234, d=0.427, g=0.737\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 152/234, d=0.428, g=0.738\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 153/234, d=0.427, g=0.740\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 154/234, d=0.426, g=0.741\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 155/234, d=0.418, g=0.742\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 156/234, d=0.426, g=0.744\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 157/234, d=0.413, g=0.745\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 158/234, d=0.418, g=0.746\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 159/234, d=0.417, g=0.748\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 160/234, d=0.411, g=0.749\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 161/234, d=0.405, g=0.751\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 162/234, d=0.407, g=0.752\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 163/234, d=0.402, g=0.754\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 164/234, d=0.403, g=0.755\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 165/234, d=0.401, g=0.757\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 166/234, d=0.393, g=0.759\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 167/234, d=0.396, g=0.761\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 168/234, d=0.396, g=0.762\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 169/234, d=0.392, g=0.764\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 170/234, d=0.394, g=0.766\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 171/234, d=0.392, g=0.768\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 172/234, d=0.394, g=0.770\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 173/234, d=0.390, g=0.772\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 174/234, d=0.390, g=0.774\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 175/234, d=0.378, g=0.776\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 176/234, d=0.380, g=0.778\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 177/234, d=0.382, g=0.780\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 178/234, d=0.379, g=0.783\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      ">1, 179/234, d=0.372, g=0.785\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 180/234, d=0.371, g=0.787\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 181/234, d=0.368, g=0.790\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 182/234, d=0.375, g=0.792\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 183/234, d=0.370, g=0.795\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 184/234, d=0.367, g=0.798\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 185/234, d=0.359, g=0.800\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 186/234, d=0.359, g=0.803\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 187/234, d=0.354, g=0.806\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 188/234, d=0.360, g=0.809\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      ">1, 189/234, d=0.350, g=0.812\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_real_samples()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 167\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m    165\u001b[0m y_gan \u001b[38;5;241m=\u001b[39m ones((n_batch, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# update the generator via the discriminator's error\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_gan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_gan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# summarize loss on this batch\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, d=\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m, g=\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, bat_per_epo, d_loss, g_loss))\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:2787\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2783\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2784\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2785\u001b[0m     )\n\u001b[1;32m   2786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2787\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/GitHub/atlas-gan/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example of training a gan on mnist\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.00005, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    " \n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.00005, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    " \n",
    "# load and prepare mnist training images\n",
    "def load_real_samples():\n",
    "    # load mnist dataset\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create and save a plot of generated images (reversed grayscale)\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "        # save plot to file\n",
    "        filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "        pyplot.savefig(filename)\n",
    "        pyplot.close()\n",
    " \n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n",
    " \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # create training set for the discriminator\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            # update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "            # evaluate the model performance, sometimes\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    " \n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "Keras version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
